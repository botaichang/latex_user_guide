\documentclass{beamer}[10]
\usepackage{texpower}
\usepackage{ucs}
\usepackage{ctex}
%\usepackage{multicol}
%\usepackage{multirow}
%\usetheme{Warsaw}
%\usetheme{Goettingen}
\usetheme{PaloAlto}
\definecolor{kugreen}{RGB}{132,158,139}
\definecolor{kugreen}{RGB}{214, 100 ,0}
%\usecolortheme{beaver}     % use white-grey colour style
\usecolortheme[named=kugreen]{structure}     % use white-grey colour style

% This is only inserted into the PDF information catalog. Can be left out.
\subject{presentation}
\keywords{example}

\title{Mathematics behind GAN}
\author{Li Jun}


\begin{document}
\begin{frame}
 \titlepage  
\end{frame}

\section*{Outline}
\begin{frame}
  \frametitle{Contents}
  \tableofcontents
  %\tableofcontents[hidesubsections,sections={<1-4>}]
\end{frame}

\section{What is GAN?}
\begin{frame}
  \frametitle{Math behind GAN}
  \begin{definition}
    GAN is composed of two networks: Descrimitive Network, and Generative Network.
  \end{definition}
\end{frame}

\section{GAN Abstract}
\begin{frame}
 \frametitle{GAN abstract}   
 \begin{enumerate}
   \item<1->  GAN is a framework for estimating generative models via an \emph{adversarial process}
   \item<2->  simultaneously train two models: A \emph{generative} model G and A \emph{discriminative} model D. 
   \item<3->  This framework corresponds to a minimax two-player game. 
 \end{enumerate}
\end{frame}

\section{Previous Generative Models}
\begin{frame}
 \frametitle{Previous Generative Models}   
 \begin{enumerate}
   \item<1->  deep Boltzmann machine  
   \item<2->  Generative stochastic networks 
   \item<3->  variational autoencoders(VAEs) 
   \item<4->  $\cdots$
 \end{enumerate}
\end{frame}


\section{Mathematics for Adversarial nets}
\begin{frame}
  \frametitle{Mathematics for Adversarial nets}
  \begin{block}{Generator}
  \begin{enumerate}
    \item data $x$
    \item input noise variables $p_z(z)$
    \item mapping to data space as $G(z;\theta_g)$ ,where G is a differentiable funciton represented by a multilayer perceptron with parameter $\theta_g$.
  \end{enumerate}
\end{block}
\end{frame}

\begin{frame}
  \frametitle{Mathematics for Adversarial nets 2}
  \begin{block}{Discriminator}
  \begin{enumerate}
    \item $D(x;\theta_d)$ which is a multilayer perceptron that outputs a single scalar.
    \item $D(x)$ represents the probability that x came from data rather than $p_g$
  \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Mathematics for Adversarial nets 3}
  \begin{block}{minimax playgame}
    $\min \limits_{G} \max \limits_{D} V(D,G) = E_{x \sim p_{data}}[\log D(x)] + E_{z \sim p_z}[\log (1 - D(G(z))] $
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Mathematics for Adversarial nets 4}
  \begin{block}{Optimium D }
    $\max \limits_{D} V(D,G) = E_{x \sim p_{data}}[\log D(x)] + E_{z \sim p_z}[\log (1 - D(G(z))] $ 
                            $= E_{x \sim p_{data}}[\log D(x)] + E_{x \sim p_g}[\log (1 - D(x)]     $ 
			    $= \int_{x} p_{data}(x)[\log D(x)] dx + \int_{x} p_g(x) \log (1 - D(x)) dx $
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Mathematics for Adversarial nets 4}
  \begin{block}{Optimium D }
    \begin{enumerate}
      \item $\max \limits_{D} V(D,G) = \int_{x} p_{data}(x)[\log D(x)] dx + \int_{x} p_g(x) \log (1 - D(x)) dx $
      \item for given x,  $p_{data}(x)$ is constant,marked as $a$   
      \item for given x, $p_g(x)$ is constant,marked as  $b$  
      \item $f(D) = a \log D + b \log(1-D)$
      \item To find max of $f(D)$, $ \frac{\partial f(D)}{\partial D} = 0 $
      \item We get $D = \frac{a}{a+b} $
      \item That is,for given G, $D^\star = \frac{p_{data}(x)}{p_g(x) + p_{data}(x)}$
    \end{enumerate}
  \end{block}
\end{frame}

\section{What is KL divergence?}
\begin{frame}
  \frametitle{KL divergence}
  \begin{definition}
   $KL(p||q) = \sum_{k=1}^{N} p_k \log \frac{p_k}{q_k} $
  \end{definition}

  \begin{block}{What's the mean of KL divergence}
     the divergence (distance) of two distributions. 
  \end{block}
\end{frame}

\end{document}
